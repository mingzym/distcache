
     _          _                   _
  __| |o ___ __| |__ ___  __ _  ___| |__   ___     distcache
 / _` |_/ __|__   __/ __|/ _` |/ __| '_ \ / , \    Distributed session caching
| (_| | \__ \  | | | (__| (_| | (__| | | |  __/    www.distcache.org
 \__,_|_|___/  |_|  \___|\__,_|\___|_| |_|\___|    distcache.sourceforge.net

-------------------------------------------------------------------------------

                                  ``Violent means will give violent freedom.''
                                                            -- Ghandi


API information
---------------

The 'distcache' package has two essentially separate APIs with the second being
divided into two libraries. The first API is the "NAL" API, which is short for
Network Abstraction Library. This library ("libnal") and API provides a few
wrapper types and functions on those types to support non-blocking network
functionality. The second API is the "distcache" API that provides the necessary
definitions to implement and/or interact with the distributed session caching
architecture. The API is split in to two libraries; "libdistcache" which
provides the encoding functionality for the network protocol used in the
architecture, as well as providing "client" interfaces for writing cache
clients, proxies, or servers. "libdistcacheserver" additionally provides
higher-level support for painlessly implementing session caching servers.

The API documentation will thus concentrate on the three libraries separately
and in the afore-mentioned order.


"libnal" - Network Abstraction Library
--------------------------------------

The types and functions provided by the "libnal" library are declared in two
header files;

 o libnal/common.h
     Type-safe macros, common system header inclusions, and various wrapper
     functions for sub-second timing logic, etc. This header exists mainly for
     the code used to *compile* the 'distcache' libraries and is only exposed in
     case it proves useful to application code. It is not necessary to use or
     include this header to work with the 'distcache' libraries.

 o libnal/nal.h
     This header declares the types and functions used to access the
     non-blocking network functionality. The rest of the "libnal" documentation
     will discuss only types and functions from this header file.


--- The NAL_ADDRESS type ---

The first abstraction one must become familiar with in "libnal" is
'NAL_ADDRESS'. This type encapsulates a means of providing a network address for
use in two possible ways; (i) "listening", ie. a socket that listens for, and
can accept, incoming connection attempts, and (ii) "connecting", ie. the address
of a listening socket to attempt to connect with. The NAL_ADDRESS abstraction
covers both of these responsibilities, and is required in the construction of
either of the two corresponding "libnal" types (NAL_LISTENER and NAL_CONNECTION,
which will be mentioned further down).

One of the major benefits of the NAL_ADDRESS abstraction is for it to
transparently deal with different network transports - at the time of writing,
NAL_ADDRESS provided support for TCP streams over both the IPv4 (Internet
Protocol) and unix domain protocols. The NAL_ADDRESS functions allow all
addresses to be specified in a single string-based namespace such that the
application needn't from that point on take any special measures depending on
what transport was involved. For example, the "dc_client" utility of 'distcache'
provides a proxy for distributed session caching that can operate on, say, a
web-server machine to handle all local session caching operations on behalf of
the web-server. It might be executed by a command-line such as;

    # dc_client -listen UNIX:/tmp/scache -server IP:cache-server.localnet:9123

in both the UNIX and IP cases (IP is an alias for IPv4), the strings are passed
verbatim to the single API function;

   int   NAL_ADDRESS_create(NAL_ADDRESS *addr,
                            const char *addr_string,
                            unsigned int def_buffer_size);

'addr' is a previously allocated NAL_ADDRESS structure (from the
NAL_ADDRESS_malloc() API function), and 'addr_string' is the
string-representation of the desired address. The return value is zero for
failure, non-zero for success. 'def_buffer_size' will dictate the default buffer
size of any connections generated from the subsequent address *or* the default
size of buffers for any connections accepted by any listeners created from the
address.

Notes on the address name-space - currently the UNIX/IP/IPv4 prefixes are
required (IPv6 will be added soon), though it is possible that the functionality
may be improved to parse a string more intelligently, guessing the transport
from the string format - however, use of the prefixes removes ambiguities anyway
so it will remain recommended behaviour. In the case of IPv4, hostnames can be
used instead of dotted-numeric IP addresses in which case a DNS lookup will
immediately be performed to resolve the hostname. The use of a port number is
always required, but the hostname/IP-address is optional if the address is only
to be used for listeners. If a NAL_ADDRESS_create() function succeeds, the
resulting address can be tested for capabilities using the following functions;

   int   NAL_ADDRESS_can_connect(NAL_ADDRESS *addr);
   int   NAL_ADDRESS_can_listen(NAL_ADDRESS *addr);

Eg. the following strings illustrate NAL_ADDRESS operation with IPv4;

   string               transport     can_connect?    can_listen?
   --------------------------------------------------------------
   IP:9001                IPv4             no             yes
   IPv4:192.168.0.1:80    IPv4             yes            yes
   IPv4:127.0.0.1:9001    IPv4             no             yes

It should be noted that "IPv4:9001" is equivalent to "IPv4:0.0.0.0:9001" with
the exception that the former can't be used for connecting, whereas the latter
can (though would always fail to connect). "IPv4:9001" means listen on port 9001
on all local interfaces, whereas "IPv4:localhost:9001" means listen on port 9001
just on the localhost interface.

The remaining NAL_ADDRESS functions are relatively self-evident and won't be
documented here.


--- The NAL_SELECTOR type ---

This type encapsulates the system select() functionality. It allows a program to
block, either for a configurable length of time or indefinately, waiting for one
of arbitrarily many different network "events" to occur. Let's first assume
NAL_SELECTOR as some kind of black-box administrator and mention the API
functions that allow you to "register" connections and listeners with the
selector and perform a 'select';

   int   NAL_SELECTOR_add_conn(NAL_SELECTOR *sel,
                               const NAL_CONNECTION *conn);
   int   NAL_SELECTOR_add_conn_ex(NAL_SELECTOR *sel,
                               const NAL_CONNECTION *conn,
                               unsigned int flags);
   int   NAL_SELECTOR_del_conn(NAL_SELECTOR *sel,
                               const NAL_CONNECTION *conn);
   int   NAL_SELECTOR_add_listener(NAL_SELECTOR *sel,
                                   const NAL_LISTENER *list);
   int   NAL_SELECTOR_del_listener(NAL_SELECTOR *sel,
                                   const NAL_LISTENER *list);
   int   NAL_SELECTOR_select(NAL_SELECTOR *sel,
                             unsigned long usec_timeout,
                             int use_timeout);

Suppose we have code with the following statements (ignoring error-checking);

   NAL_SELECTOR_add_listener(sel, list);
   NAL_SELECTOR_add_conn(sel, conn1);
   NAL_SELECTOR_add_conn(sel, conn2);
   NAL_SELECTOR_select(sel, 0, 0);

The NAL_SELECTOR_select() statement would block indefinitely until one of the
three objects, 'list', 'conn1', and/or 'conn2', experienced a "useful network
event". A useful network event would mean;

 o for 'list', the arrival of an incoming connection attempt
 o for 'conn1', the arrival of new data *ONLY* if the input buffer for 'conn1'
   wasn't already full, or the ability to send data *ONLY* if the output buffer
   for 'conn1' wasn't empty.
 o 'conn2' similarly to 'conn1'

What's most important about the NAL_SELECTOR logic is that it essentially has
two selector states in one. Once the NAL_SELECTOR_select() function returns,
there are two noticable changes of state in 'sel';

  (i) 'sel' will maintain state indicating *which* listeners and/or connections
      had useful network events worth processing. This comes in useful later
      when looking at I/O functions for NAL_CONNECTIONs and "accept" functions
      for NAL_LISTENERs. Without prematurely discussing the NAL_CONNECTION
      functions, the NAL_CONNECTION_io() command takes not only the connection
      object, but a selector object too that will use this information to know
      what data transfers it can and can't do based on the last select.
 (ii) The internal state used for NAL_SELECTOR_select() is wiped clean so that
      while the results of the last select are being used (I/O with connections,
      accept connections on listeners, etc), one can begin adding connections
      and listeners to the selector at the same time for use in the *next*
      select operation. This is what allows the non-blocking logic to support a
      state-machine model in application code.

Extra notes - the 'flags' parameter to NAL_SELECTOR_add_conn_ex() allows the
caller to explicitly screen out certain network events on the given connection
object. Eg. if the connection has data sitting in its output buffer but the
caller does *not* want the select to break when the network allows data to be
sent, the caller would pass NAL_SELECT_FLAG_READ as a flag value (ie. without
bitwise-ORing NAL_SELECTOR_FLAG_SEND as would be the normal behaviour). The
NAL_SELECTOR_del_conn() and NAL_SELECTOR_del_listener() functions are only
needed if you've previously added an object to the selector but want to "unadd"
it before the next select - after a select, the list of objects/events to be
selected on is wiped clean anyway. The 'use_timeout' parameter in
NAL_SELECTOR_select() controls whether the 'usec_timeout' value should be used
or not. If 'use_timeout' is zero, 'usec_timeout' is ignored and the select will
block indefinitely waiting for a network event (or until it is interrupted by a
signal). Otherwise, 'usec_timeout' is the maximum number of microseconds the
select should block until it simply breaks without any network events having
arrived. Note that on most systems, this will not actually occur with
microsecond-accuracy, the host system will provide the best approximation it
can.


--- The NAL_LISTENER type ---

This type can be created from a NAL_ADDRESS for which NAL_ADDRESS_can_listen()
returns TRUE, although it can fail for various run-time environmental reaons.
But first the critical function;

   int   NAL_LISTENER_create(NAL_LISTENER *list,
                             const NAL_ADDRESS *addr);

'list' is a NAL_LISTENER previously allocated by calling NAL_LISTENER_malloc()
and NAL_ADDRESS is the address on which we wish to set up a listening socket.
The return value is zero for failure, non-zero for success. There are various
reasons why this function could fail;

 o The address isn't suitable for listening, see NAL_ADDRESS_can_listen().
 o The address is for unix domain sockets and specifies an invalid path; eg.
    UNIX:/this/path/does/not/exist
 o The address is for unix domain sockets and the current user account does not
   have the necessary permissions for the address's path.
 o The address is for IPv4 and specifies an invalid port.
 o The address is for IPv4, and the port value requires the special user
   privileges.
 o The address is for IPv4, specifies a hostname or IP address, and the
   hostname/IP address is not a local interface.
 o Attempting to listen on the address conflicts with a service already running
   and listening.

Working out at run-time *which* of these reasons is responsible for a run-time
failure is currently not provided, but is quite likely to be developed in the
near future.

Once a listener is created successfully, its use is covered by 3 API functions;

   int   NAL_SELECTOR_add_listener()   (mentioned above in NAL_SELECTOR)
   int   NAL_LISTENER_accept(const NAL_LISTENER *list,
                             NAL_SELECTOR *sel,
                             NAL_CONNECTION *conn);
   int   NAL_LISTENER_accept_block(const NAL_LISTENER *list,
                                   NAL_CONNECTION *conn);

The first "accept" function will use the results of the last
NAL_SELECTOR_select() operation on 'sel' to allow it to accept any waiting
connection attempt to the listener 'list' or know immediately that there are no
pending connection attempts to accept. This function is therefore
"non-blocking". The second "accept" function doesn't take any selector and will
simply wait until a connection to 'list' arrives or the function is interrupted
by a signal. In either case, either function will return non-zero to indicate
success, in which case 'conn' (which must have been allocated by
NAL_CONNECTION_malloc() but not be currently active) will represent the accepted
connection. Likewise, if either function returns zero, 'conn' is left untouched
and no connection is accepted from 'list'.


--- The NAL_CONNECTION type ---

This is the most complicated of the libnal types, mainly because it is both a
network entity and a container for two data buffers (or queues, or FIFOs if you
prefer). A conventional connection is established using the following API
function;

   int   NAL_CONNECTION_create(NAL_CONNECTION *conn,
                               const NAL_ADDRESS *addr);

The return value of this function is zero for failure, non-zero for success.
However, as the libnal library is designed to be entirely non-blocking, this
function does not necessarily guarantee a successful connection to the target
address is the function returns non-zero, for the simple reason that a TCP
connection (over IPv4 at least) has not completed by the time this function
returns. For unix domain sockets, whether this is the case or not probably
depends on the system you're using, and perhaps the C library too. A successful
result does at least say that the system has approved the connection initiation
attempt and that it was in progress at the time the function returned. An extra
function is used to determine when the libnal library knows if the connection is
established or not;

   int   NAL_CONNECTION_is_established(const NAL_CONNECTION *conn);

However, due to the model in use - this function may well not know whether the
connection attempt has completed or not until the first attempt to read or write
data has succeeded or not.

That subtlety aside, we will consider connections that have been successfully
created as above. Inside each NAL_CONNECTION object, are two buffers; one for
input and one for output. These buffers can be accessed by the two API
functions;

   NAL_BUFFER *   NAL_CONNECTION_get_read(NAL_CONNECTION *conn);
   NAL_BUFFER *   NAL_CONNECTION_get_send(NAL_CONNECTION *conn);

The NAL_BUFFER functions can then be used to write data to the send buffer and
read data from the read buffer. The converse is also possible though one would
start to ask oneself the question: why try?! The answer should be: don't, the
converse is only useful inside the implementation of NAL_CONNECTION logic
performing the underlying network I/O.

Data written to a NAL_CONNECTION object is not actually sent out on the network
immediately, and likewise data that has arrived from the network is not
automatically available for reading in the connection object. This is where the
NAL_SELECTOR logic (mentioned above) becomes involved - when a NAL_CONNECTION
object is added to a selector and a select operation is performed, the selector
object will have internal state indicating whether any data written to the
connection's send buffer can actually now be sent out over the network. It will
also know if there is data from the network waiting to be placed into the
connection's read buffer. To make the connection flush outgoing data if the
selector allows it, and to have the connection read any available incoming data
that it has space for, use one of the following API functions;

   int NAL_CONNECTION_io(NAL_CONNECTION *conn,
                         NAL_SELECTOR *sel);
   int NAL_CONNECTION_io_cap(NAL_CONNECTION *conn,
                             NAL_SELECTOR *sel,
                             unsigned int max_read,
                             unsigned int max_send);

The first function will transparently "take care of everything"; anything that
was in the send buffer that could be sent, would have been, and anything that
arrived over the network that could fit in the read buffer would have been
placed there. The "_cap" version simply allows a maximum value to be placed on
how much of any outgoing data (in the send buffer) is allowed to be sent this
time and how much of any incoming data is allowed to be absorbed into the read
buffer - this is of limited use but provided "just in case".

In fact, there are various "special" types of connections that can also be
created in NAL_CONNECTION objects that we will now cover. The first is the
concept of a "connection pair" - this means two NAL_CONNECTION objects that have
no inherent NAL_ADDRESS, but rather act as terminal points to each other. Data
"sent" from one connection will "arrive" at the other. A connection pair is
created using the following API function;

   int   NAL_CONNECTION_create_pair(NAL_CONNECTION *conn1,
                                    NAL_CONNECTION *conn2,
                                    unsigned int def_buffer_size);

The reason for 'def_buffer_size' is that the default size for the read and send
buffers of a connection are usually inherited from the NAL_ADDRESS object that
was used to create a connection - in this case that is clearly not possible. NB:
where it is possible to do so, the connection pair will be created with real
system sockets (eg. the socketpair() function found on most unix varieties). One
benefit of this is to automatically create an IPC mechanism between threads or
processes. The most common use is IPC between a parent process and a child
process after a fork() statement - the connection pair is created then fork() is
called, afterwards the parent process will close one of the connection objects
and use the other to communicate with the child, the child will do the same but
with the order of objects reversed.

Another special connection type, though arguably less practical, is that of a
"dummy" connection. This connection is essentially an "echo" or "loopback"
device with the read and send buffers being the same structure. Data written to
the connection goes into the one buffer so that reads will pull out the
corresponding data. Whilst this might seem a pointless feature, the fact it is
possible to support this entirely inside the NAL_CONNECTION abstraction should
make it easier in some circumstances to test any application code using a
connection that expects data to be bounced back to it from its peer. A dummy
connection is created with the following API function;

   int   NAL_CONNECTION_create_dummy(NAL_CONNECTION *conn1,
                                     unsigned int def_buffer_size);

The remaining points to mention involve control of the buffer sizes of a
connection object. As mentioned, a convention connection object will inherit the
default buffer size from the NAL_ADDRESS object that created it or by having it
directly supplied in the corresponding NAL_CONNECTION API function. To change
the size of both the read and send buffers after the connection has been
created, the following API function exists;

   int   NAL_CONNECTION_set_size(NAL_CONNECTION *conn1,
                                 unsigned int size);

However, the buffers can be individually resized as well by using the NAL_BUFFER
API functions directly on the buffers (as returned by NAL_CONNECTION_get_read
and NAL_CONNECTION_get_send).


--- The NAL_BUFFER type ---

This is the buffer type used inside the NAL_CONNECTION type to handle data I/O.
The NAL_CONNECTION functions allow access to the individual read and send
buffers of a connection object, so the NAL_BUFFER functions can be used to
subsequently operate on these to perform tests, data input/output, and change
the buffer size. The following API functions compare the current size of a
buffer object with the amount of data it is currently holding;

   int   NAL_BUFFER_empty(const NAL_BUFFER *buf);
   int   NAL_BUFFER_full(const NAL_BUFFER *buf);
   int   NAL_BUFFER_notempty(const NAL_BUFFER *buf);
   int   NAL_BUFFER_notfull(const NAL_BUFFER *buf);
   unsigned int   NAL_BUFFER_used(const NAL_BUFFER *buf);
   unsigned int   NAL_BUFFER_unused(const NAL_BUFFER *buf);

In the first four cases, the return value is a boolean result where zero is
FALSE and non-zero is TRUE. In the latter two respectively, it returns the
amount of data in the buffer and the amount of unused space (for storing any new
data). The size of a buffer (the maximum amount of data it can store) can be
obtained and set by the following two API functions;

   unsigned int   NAL_BUFFER_size(const NAL_BUFFER *buf);
   int   NAL_BUFFER_set_size(NAL_BUFFER *buf,
                             unsigned int size);

Note that care should be taken when changing the size of a buffer that already
contains a non-zero quantity of data - if the new size is less than the amount
of data in the buffer, the data will be cropped to the new size - thus data will
be lost and the buffer will be full upon returning.

Writing and reading data to/from the buffer object is usually via the following
two API functions;

   unsigned int   NAL_BUFFER_write(NAL_BUFFER *buf,
                                   const unsigned char *ptr,
                                   unsigned int size);
   unsigned int   NAL_BUFFER_read(NAL_BUFFER *buf,
                                  unsigned char *ptr,
                                  unsigned int size);

Note however that the buffer will not necessarily absorb (for "write") or
produce (for "read") 'size' bytes of input/output - 'size' is simply a maximum
value. The return value will indicate how much exactly was written or read and
so the caller should very carefully check this value and adapt appropriately if
it is less than 'size'.

In addition to the above functions, there are other NAL_BUFFER functions that
can be used by calling applications when they want finer control over buffer
operation, however they come with various conditions. These functions often need
to be used in sequences and assume that no other operations are taking place on
the same buffer in the interim. Also, it is often not possible for these
functions to perform enough internal sanity checking so calling applications
need to be careful.

The first two special functions are;

   unsigned char *   NAL_BUFFER_read_ptr(NAL_BUFFER *buf);
   unsigned char *   NAL_BUFFER_write_ptr(NAL_BUFFER *buf);

The first returns a pointer to the head of the buffer's internal data array.
This data array is of size NAL_BUFFER_size(buf), and the amount of the array
populated with data is NAL_BUFFER_used(buf). The second function returns a
pointer *into* the buffer's internal data array, pointing to the first unused
byte. Ie. it is offset NAL_BUFFER_used(buf) from the start of the data array and
there exists NAL_BUFFER_unused(buf) bytes (possibly equal to zero) of the array
remaining from the returned pointer.

The other two special functions used with the above are;

   unsigned int   NAL_BUFFER_takedata(NAL_BUFFER *buf,
                                      unsigned char *dest,
                                      unsigned int size);
   unsigned int   NAL_BUFFER_wrote(NAL_BUFFER *buf,
                                   unsigned int size);

The first is function has exactly the same function and semantics as
NAL_BUFFER_read() provided 'dest' is non-NULL. If 'dest' is NULL, then this
simply scrolls the first 'size' bytes out of the buffer's internal data array.
This can be useful if an application has used the result of
NAL_BUFFER_read_ptr() to read and consume as much of the buffer's data as it
wants - it can simply pass 'size' with a NULL 'dest' and this data will be
discarded from the buffer and any remaining data will be scrolled forward. The
second function is for use when an application has written data to the pointer
returned from NAL_BUFFER_write_ptr() - it simply tells the buffer to acknowledge
that 'size' more bytes of its data array are now used than before.

All of these functions should be used with care to ensure no edge-cases cause
bugs in the application. Unless the calling application mis-uses pointer values
or performs reads/writes beyond the limits provided by the NAL_BUFFER functions,
there should be no risk of buffer overflows or stack-corruption, however
applications that, for example, incorrectly assume a read or write of 'size'
bytes occurred when in fact 'size' was too large (in which case the return value
indicates the *actual* amount read or written), then application-level bugs can
occur.



"libdistcache" - Distributed Session Caching Library
----------------------------------------------------

The types and functions provided by the "libdistcache" library are declared in
two header files;

 o libdistcache/dc_enc.h
     This header essentially defines the protocol-level types and functions used
     for implementing network services that can interoperate with the
     'distcache' architecture. This implementation layer is used to implement
     client- and server-side logic, and the remaining 'distcache' code and
     applications build upon this layer to implement higher-level functionality.

 o libdistcache/dc_client.h
     This header declares a high-level API implemented in 'libdistcache' that
     can be used by applications wishing to use distributed session caching. An
     example of this is the session cache code used in Apache/mod_ssl - it uses
     the API exposed by dc_client.h to allow SSL/TLS sessions to be added,
     removed, and queried to/from the distributed session cache.

"libdistcache/dc_enc.h", and the lower-level API it declares, will not be
covered here for now. Use of that API is not generally advised as most "client"
needs can be met by the API exposed by "dc_client.h", and likewise most "server"
needs are presented by the 'libdistcacheserver' library.


--- The DC_CTX type ---

The abstraction provided by "dc_client.h" centres around the DC_CTX type. This
type is associated with a target address string during creation and will remain
associated to that address throughout its use. This target address is typically
the address of a proxy service running on the same local machine, thus the use
of unix domain addresses (eg. "UNIX:/tmp/cache_proxy") or localhost IPv4
addresses (eg. "IP:localhost:9001") is common. This context type allows the
calling application to request operations on the distributed cache such as; the
addition of new sessions, the retrieval or removal of cached sessions, and
querying the existence of a session (without actually retrieving it).

Sessions in this architecture equate to key-value pairs where the key itself is
a unique binary string of length no greater than DC_MAX_ID_LEN (which is defined
in the lower-level "dc_enc.h" header file, equal to 64 at the time of writing).
There can only be one session in the distributed cache corresponding to any key.
Like keys, values are also binary strings, though are not necessarily globally
unique and can typically be much larger than keys - the limit on the length of a
value is DC_MAX_TOTAL_DATA, which was 32 kilobytes at the time of writing.

Eg. for SSL/TLS session caching with OpenSSL-based applications (such as Apache
2.0, or Apache 1.3.x with mod_ssl), the values used in the distributed session
caching framework are DER-encoded SSL_SESSION objects from the OpenSSL API. The
keys used to refer to these sessions are the SSL/TLS "session id" values which
by design are unique identifiers for a session (and contain no
security-sensitive information).

DC_CTX context objects are created and destroyed by the following functions;

   DC_CTX *   DC_CTX_new(const char *target, unsigned int flags);
   void       DC_CTX_free(DC_CTX *ctx);

The meaning of DC_CTX_free() should be evident. For DC_CTX_new(), the 'target'
network address of the local distributed session cache proxy (or
it could be a session cache server, though this mode of operation is not
recommended in most circumstances). This network address is supplied in a string
form compatible with 'libnal', as described earlier. 'flags' is a bitwise-OR'd
mask of settings to control how the DC_CTX object should function when handling
its networking responsibilities internally. These flags are enumerated and
described here;

   #define DC_CTX_FLAG_PERSISTENT               (unsigned int)0x0001
   #define DC_CTX_FLAG_PERSISTENT_PIDCHECK      (unsigned int)0x0002
   #define DC_CTX_FLAG_PERSISTENT_RETRY         (unsigned int)0x0004
   #define DC_CTX_FLAG_PERSISTENT_LATE          (unsigned int)0x0008

If DC_CTX_FLAG_PERSISTENT is not defined, then the DC_CTX has only one mode of
operation - it will open a temporary connection to the target service for each
operation and close it immediately when finished. In the other case, the DC_CTX
will attempt to use a persistent connection, and the other flags provide various
modifications to behaviour;

   DC_CTX_FLAG_PERSISTENT_PIDCHECK
      This flag will cause the DC_CTX operations to internally check that the
      process ID hasn't changed since last time it performed an operation - if
      it has changed, then any persistent connection already open will be closed
      and re-opened. This is intended to help in applications where fork() is
      used to create child processes - otherwise child processes could
      simultaneously use the same connection and cross-corrupt each other.
   DC_CTX_FLAG_PERSISTENT_RETRY
      This flag allows DC_CTX operations to work with distributed cache proxies
      (or servers if connecting directly) that time-out idle connections. It
      accomplishes this by allowing each operation to tolerate one disconnection
      error on the persistent connection by reconnecting and restarting.
   DC_CTX_FLAG_PERSISTENT_LATE
      This flag allows the DC_CTX object to be created without creating a
      persistent connection immediately. Normally, when creating a "persistent"
      DC_CTX context, the implementation will attempt to connect to the 'target'
      address and fail if the connection is not possible - this is an
      error-checking measure but in some circumstances it might be inconvenient.
      Eg. creating a "persistent" context with the _LATE flag allows an
      application to fork() child processes before doing any other DC_CTX
      operations and there would be no need for the _PIDCHECK flag.

The choice of what mode to use depends rather heavily on the expected usage
profile. The best I can do is make a couple of notes here for what they're
worth;

  - using temporary per-operation connections does add latency to each
    operation, however that overhead is normally very negligable, particularly
    if communicating to a distributed session caching proxy over unix domain
    sockets.
  - if using persistent connections, and it is possible that many could be open
    at one time (ie. different DC_CTX contexts, or one context fork()d into many
    child processes), then the local caching proxy could hit file-descriptor
    limits on the host system or at least begin to perform sub-optimally.

Once the appropriate 'flags' and 'target' have been chosen and specified to
create a DC_CTX, various run-time operations can be performed using the context.
The first allows a new session item to be added into the distributed cache;

   int   DC_CTX_add_session(DC_CTX *ctx,
                            const unsigned char *id_data,
                            unsigned int id_len,
                            const unsigned char *sess_data,
                            unsigned int sess_len,
                            unsigned long timeout_msecs);

If this returns non-zero, then the operation is presumed to have succeeded. If
not, it may have failed for various reasons; if an existing session already
exists with the same id_data/id_len pair, if there are network problems
communicating with the local cache proxy, if the local cache proxy can not
communicate with the backend cache server(s), or if the input parameters are
invalid for some reason. 'id_data' and 'sess_data' must point to valid binary
arrays of size 'id_len' and 'sess_len' respectively. Both arrays must have
non-zero length, 'id_len' must be no more than DC_MAX_ID_LEN, and 'sess_len'
must be no more than DC_MAX_TOTAL_DATA. 'timeout_msecs' is the number of
milliseconds that this session should be considered valid. It is possible for
the distributed session cache to pre-maturely expire sessions if the storage
mechanisms require room to be found for newly added sessions, but the
distributed cache will not return a session after the "timeout" period has
passed, even if it is not been explicitly expired from the cache at the time the
caller calls DC_CTX_add_session().

Two of the remaining DC_CTX operations are;

   int   DC_CTX_remove_session(DC_CTX *ctx,
                               const unsigned char *id_data,
                               unsigned int id_len);
   int   DC_CTX_has_session(DC_CTX *ctx,
                            const unsigned char *id_data,
                            unsigned int id_len);

Each takes an 'id_data'/'id_len' pair specifying the session object's key and
returns a boolean result. If the "remove" function returns non-zero, then such a
session existed in the cache and was successfully removed, otherwise it failed
due to specifying a non-existent session or some network error. If the "has"
session returns non-zero then such a session exists in the cache (the operation
does not retrieve the session, just checks for its existence).

The remaining functionality for DC_CTX is the retrieval of sessions from the
distributed cache. There are two functions associated with this task, though the
second is only required in certain circumstances;

   int   DC_CTX_get_session(DC_CTX *ctx,
                            const unsigned char *id_data,
                            unsigned int id_len,
                            unsigned char *result_storage,
                            unsigned int result_size,
                            unsigned int *result_used);
   int   DC_CTX_reget_session(DC_CTX *ctx,
                              const unsigned char *id_data,
                              unsigned int id_len,
                              unsigned char *result_storage,
                              unsigned int result_size,
                              unsigned int *result_used);

If DC_CTX_get_session() returns zero then the operation failed - perhaps because
the session requested does not exist, or because of a network error. If it
returns non-zero, the operation is considered to have succeeded - though whether
the caller has received all the session data associated with the request depends
on the parameters passed to the function. 'result_storage'/'result_size'
specifies a data buffer for the function to populate with the retrieved session
data - however it is possible that the session data does not entirely fit in the
provided buffer (in fact 'result_storage' can be set to NULL to force precisely
this situation). If that is the case, the length of the session data for the
requested session is stored in the 'result_used' value.

The entire 'distcache' package employs a variety of optimisations and caching
tricks, but one in particular relates to the above functions. When processing
the DC_CTX_get_session() function, the session data retrieved is buffered
internally in the DC_CTX object whether the caller provided appropriate space to
store it or not. If the caller didn't provide enough space, then they can use
the value set in 'result_used' to find enough space and then call
DC_CTX_reget_session() with the *identical* 'id_data'/'id_len' values as were
used in DC_CTX_get_session(). NB: This only works if no other operation has
taken place on the DC_CTX between the two calls, and requires that exactly the
same 'id_data'/'id_len' key value is passed to the "reget".


--- Other uses for the 'dc_client.h' API ---

'distcache' is not restricted to SSL/TLS, any other "state" storage problem
could be a candidate for using 'distcache' if the following model applies; each
item has a corresponding short binary string that uniquely identifies it, each
item's data is a binary string or can be encoded/decoded to/from such a
representation, multiple systems must share a consistent database of such items
between them.
